{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "![WMLE LOGOS](https://github.com/sanjayksau/wmle2024/blob/main/logo3.png?raw=true)"
      ],
      "metadata": {
        "id": "dNQhdrI2Ldd8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#XGBoost Classifier\n",
        "In this workbook, we will use XGBoost, a powerful machine learning algorithm, to predict credit risk using the German Credit Dataset. The task is to classify whether a credit applicant is a \"Good Credit\" or a \"Bad Credit\" based on various financial and personal attributes.\n",
        "https://xgboost.readthedocs.io/en/latest/tutorials/model.html\n",
        "\n",
        "https://www.nvidia.com/en-in/glossary/xgboost/\n"
      ],
      "metadata": {
        "id": "az9cJhnt7BFm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Install Required Libraries"
      ],
      "metadata": {
        "id": "rb0VBy3K7I3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#install xgboost\n",
        "!pip install xgboost"
      ],
      "metadata": {
        "id": "G-ISLkPJ7QsN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "\n",
        "#suppress warnings\n",
        "#import warnings\n",
        "#warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "9Bqt2uda7LVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load the dataset\n",
        "The dataset we will use is the German Credit Dataset, which can be found here. It contains data about applicants who applied for credit, along with information about their financial status."
      ],
      "metadata": {
        "id": "_GlbsFmc7XbJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the German Credit dataset\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data\"\n",
        "\n",
        "# Define column names for the dataset\n",
        "columns = [\n",
        "    'Status of existing checking account', 'Duration in month', 'Credit history',\n",
        "    'Purpose', 'Credit amount', 'Savings account/bonds', 'Present employment since',\n",
        "    'Installment rate in percentage of disposable income', 'Personal status and sex',\n",
        "    'Other debtors / guarantors', 'Present residence since', 'Property',\n",
        "    'Age in years', 'Other installment plans', 'Housing', 'Number of existing credits at this bank',\n",
        "    'Job', 'Number of people being liable to provide maintenance for', 'Telephone',\n",
        "    'foreign worker', 'Creditability'  # The target column (1: Good, 2: Bad)\n",
        "]\n",
        "\n",
        "#TODO: Read the dataset but with custom/meaningful feature/columns names.\n",
        "#Read the dataset (separator space, no header, columns as names) into a pandas DataFrame.\n",
        "\n",
        "# TODO: Display the first few rows of the dataset\n"
      ],
      "metadata": {
        "id": "wAEXdfiC7cRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Preprocessing\n",
        "Before we train our model, we need to prepare the data. This involves converting categorical data into numerical values and preparing the target labels for binary classification (1 for \"Bad Credit\" and 0 for \"Good Credit\")."
      ],
      "metadata": {
        "id": "Z-UFlxP27sRu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: Display DataFrame info\n"
      ],
      "metadata": {
        "id": "Q-lTWnJghzT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "#TODO: Convert target labels to binary (0 for Good Credit, 1 for Bad Credit)\n",
        "#Assign map to df['Creditability']\n",
        "\n",
        "# Preprocess categorical variables using LabelEncoder\n",
        "# Convert categorical features into numerical values\n",
        "for column in df.select_dtypes(include=['object']).columns:\n",
        "    #TODO: fit and transform df[column] using LabelEncoder()\n",
        "\n",
        "#TODO: Display the first few rows after preprocessing\n"
      ],
      "metadata": {
        "id": "zSx7pzRd7yjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Split the Dataset\n",
        "We will split the dataset into training and test sets to evaluate the model’s performance."
      ],
      "metadata": {
        "id": "BaTTYnXe8mSi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# TODO: Split df into features (X) and target (y), drop columns=['Creditability']\n",
        "\n",
        "\n",
        "#TODO: Split the data into training and test sets\n",
        "\n",
        "#Display the shapes of the train and test sets\n"
      ],
      "metadata": {
        "id": "-4yL1qbb8tjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train on XGBoost Model\n"
      ],
      "metadata": {
        "id": "EuT0IFIj8_cn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "# Convert the dataset into DMatrix, which is the internal data structure used by XGBoost\n",
        "\n",
        "#DMatrix are XGBoost special data structures to represent datasets in the most efficient\n",
        "#way for XGBoost\n",
        "train_data = xgb.DMatrix(X_train, label=y_train)\n",
        "test_data = xgb.DMatrix(X_test, label=y_test)\n",
        "\n",
        "# Define the parameters for the XGBoost model\n",
        "#https://xgboost.readthedocs.io/en/latest/parameter.html\n",
        "\n",
        "#“binary:logistic” –logistic regression for binary classification, output probability\n",
        "#logloss: negative log-likelihood\n",
        "params = {\n",
        "    #'objective': 'binary:hinge',\n",
        "    'objective': 'binary:logistic',  # logistic regression for binary classification, o/p probability\n",
        "    'max_depth': 4,  # Maximum depth of each tree\n",
        "    'learning_rate': 0.1,  # Learning rate\n",
        "    'eval_metric': 'logloss', # Logarithmic loss as the evaluation metric\n",
        "    'seed': 42  # Random seed for reproducibility\n",
        "}\n",
        "\n",
        "# Train the XGBoost model with 100 boosting rounds\n",
        "#train <=> fit\n",
        "model = xgb.train(params, train_data, num_boost_round=100)\n",
        "\n",
        "print(\"Model training done.\")\n",
        "\n",
        "#TODO: Try training with 'binary:hinge' and eval_metric: 'mae', mean absolute error"
      ],
      "metadata": {
        "id": "dvUBEZLd9Dnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Make Predictions and Evaluate the Model\n",
        "After training, we will use the test data to evaluate the model's performance by making predictions and calculating accuracy, confusion matrix, and classification report."
      ],
      "metadata": {
        "id": "VhP8BKQr9OW8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "#TODO: Make predictions y_pred_prob on the test data using model.predict\n",
        "\n",
        "#TODO: Convert probabilities into binary predictions (0 if <=0.5 or 1 if >0.5)\n",
        "\n",
        "#TODO: Evaluate the model's accuracy using y_test, y_pred\n",
        "print(\"Accuracy: \", accuracy_score(y_test, y_pred))\n",
        "\n",
        "#TODO: Confusion Matrix: Rows: Ground Truth, Columns: Predictions\n"
      ],
      "metadata": {
        "id": "H_tPyEwH9Y8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Evaluation Metrics:\n",
        "- What do precision, recall, and F1-score tell about the model’s performance?\n",
        "\n",
        "Accuracy measures the proportion of correct predictions(both True positives and True Negatives).\n",
        "\n",
        "$$Accuracy=\\frac{TP+TN}{TP+TN+FP+FN}$$\n",
        "\n",
        "Precision is the proportion of positive predictions that are actually correct. It is important when False positives are costly (e.g. fraud detection, spam filtering)\n",
        "$$Precision = \\frac{TP}{TP+FP}$$\n",
        "\n",
        "\n",
        "Recall is the proportion of actual positives, that the model correct identify. It is important when False negatives are costly (e.g. identifying diseases)\n",
        "$$Recall = \\frac{TP}{TP+FN}$$\n",
        "\n",
        "\n",
        "F1-Score is the harmonic mean of precision and recall. It balances both precision and recall especially when the class are imbalanced."
      ],
      "metadata": {
        "id": "uMnlnslr9oC6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Optional Tasks\n",
        "- Hyperparameter Tuning: Experiment with different XGBoost parameters (max_depth, learning_rate, etc.) to improve the model's performance.\n",
        "- Feature Engineering: Try adding or transforming features to see if it improves the model's accuracy.\n",
        "- Cross-validation: Implement cross-validation to get a more reliable estimate of the model's performance."
      ],
      "metadata": {
        "id": "2rG7EfzD9nEr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Hyperparameter Tuning using GridSearchCV\n",
        "We will use GridSearchCV to search for the best hyperparameters for the XGBoost model. First, we define the parameter grid we want to search over, and then we use GridSearchCV to find the best combination."
      ],
      "metadata": {
        "id": "AGPrPd4K-W6p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "import xgboost as xgb\n",
        "\n",
        "# Create a base model\n",
        "xgb_model = xgb.XGBClassifier(objective='binary:logistic', eval_metric='logloss', use_label_encoder=False)\n",
        "\n",
        "#Note that the tuning may drive the performance low depending on the parameter range specified.\n",
        "#So a wide range might be needed. Example below is just to highlight how to do a tuning over a\n",
        "#specified parameter range.\n",
        "\n",
        "# Define the parameter grid for GridSearch\n",
        "param_grid = {\n",
        "    'max_depth': [3, 4, 5],        # Maximum depth of the trees\n",
        "    'learning_rate': [0.01, 0.1, 0.2],  # Learning rate\n",
        "    'n_estimators': [50, 100, 200],     # Number of boosting rounds\n",
        "}\n",
        "\n",
        "# Instantiate the GridSearchCV object with 3-fold cross-validation\n",
        "#https://scikit-learn.org/dev/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
        "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=3, verbose=1) #verbose=1,2,3\n",
        "\n",
        "# Fit the grid search to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Output the best parameters found by GridSearch\n",
        "print(f\"Best parameters: {grid_search.best_params_}\")\n",
        "\n",
        "# Get the best model from grid search\n",
        "best_model = grid_search.best_estimator_\n"
      ],
      "metadata": {
        "id": "81HGRpcV6ym9",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Re-evaluate the tuned model\n",
        "After obtaining the best model from GridSearchCV, we can use it to make predictions and evaluate its performance just like we did earlier."
      ],
      "metadata": {
        "id": "6tzKKzMD-9rl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "#TODO: Make predictions with the tuned model: best_model.predict\n",
        "\n",
        "# Evaluate the tuned model's accuracy\n",
        "accuracy_tuned = accuracy_score(y_test, y_pred_tuned)\n",
        "print(f\"Accuracy after tuning: {accuracy_tuned}\")\n",
        "\n",
        "# Confusion Matrix for the tuned model\n",
        "conf_matrix_tuned = confusion_matrix(y_test, y_pred_tuned)\n",
        "print(\"Confusion Matrix (Tuned Model):\")\n",
        "print(conf_matrix_tuned)\n",
        "\n",
        "# Classification Report: Provides precision, recall, and F1-score for both classes\n",
        "# Classification Report for the tuned model\n",
        "#class_report_tuned = classification_report(y_test, y_pred_tuned)\n",
        "#print(\"Classification Report (Tuned Model):\")\n",
        "#print(class_report_tuned)\n"
      ],
      "metadata": {
        "id": "MY9vfHMc-bRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TODO:\n",
        "- Change parameters loss, eval_metric etc by refering to documentation for available options: https://xgboost.readthedocs.io/en/stable/parameter.html\n",
        "\n",
        "- Build a model using xgb.train and XGBClassifier, train and test\n",
        "(Refer xgb and XGBClassifier documentation)\n",
        "- Change verbosity level in GridSearchCV to higher value(2 or more) and observe the messages.\n",
        "- Prepare a synthetic dataset using make_classification with two features and two classes. Use XGB for eval and compare the results with DecisionTree"
      ],
      "metadata": {
        "id": "J8tgk7euPKl9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b1DdTB4yRWFG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}